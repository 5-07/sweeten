{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# ðŸš€ Sweeten â€“ Colab Local Mode Setup\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook sets up **Ollama** inside Colab, pulls `gpt-oss` model, and exposes it with **ngrok** so your Next.js app can connect as if it were running locally.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# 1. Install Ollama\\n\",\n",
    "    \"!curl -fsSL https://ollama.com/install.sh | sh\\n\",\n",
    "    \"!ollama serve &\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# 2. Pull a model\\n\",\n",
    "    \"# Try gpt-oss:20b (quantized fits in 16GB VRAM T4)\\n\",\n",
    "    \"!ollama pull gpt-oss:20b\\n\",\n",
    "    \"\\n\",\n",
    "    \"# If it fails due to memory, fallback:\\n\",\n",
    "    \"# !ollama pull phi:2.7b\\n\",\n",
    "    \"# !ollama pull tinyllama:1.1b\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# 3. Install ngrok\\n\",\n",
    "    \"!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\\n\",\n",
    "    \"!unzip ngrok-stable-linux-amd64.zip\\n\",\n",
    "    \"!./ngrok authtoken YOUR_NGROK_AUTH_TOKEN\\n\",\n",
    "    \"!./ngrok http 11434 &\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# 4. Warm-up test: simple JSON prompt\\n\",\n",
    "    \"import requests, json\\n\",\n",
    "    \"resp = requests.post(\\\"http://localhost:11434/api/generate\\\", json={\\n\",\n",
    "    \"    \\\"model\\\": \\\"gpt-oss:20b\\\",\\n\",\n",
    "    \"    \\\"prompt\\\": \\\"Reply with JSON: {\\\\\\\"ok\\\\\\\": true}\\\",\\n\",\n",
    "    \"    \\\"stream\\\": False\\n\",\n",
    "    \"})\\n\",\n",
    "    \"print(resp.json())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### ðŸ“‹ Instructions\\n\",\n",
    "    \"1. Replace `YOUR_NGROK_AUTH_TOKEN` in cell 3 with your token from https://ngrok.com.\\n\",\n",
    "    \"2. Run all cells in order.\\n\",\n",
    "    \"3. After cell 3, ngrok will print a **Forwarding URL** like `https://abcd-1234.ngrok.io`.\\n\",\n",
    "    \"4. Copy that URL into your Next.js `.env.local`:\\n\",\n",
    "    \"   ```env\\n\",\n",
    "    \"   OLLAMA_MODEL=gpt-oss:20b\\n\",\n",
    "    \"   OLLAMA_HOST=https://abcd-1234.ngrok.io\\n\",\n",
    "    \"   ```\\n\",\n",
    "    \"5. In your app, toggle **Local Mode ON** â†’ it will call Colab Ollama.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Thatâ€™s it. Judges see you flip the toggle â†’ plan comes from your Colab runtime.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
